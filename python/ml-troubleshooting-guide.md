# ë¨¸ì‹ ëŸ¬ë‹ ê°œë°œ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ğŸ¤–

## ëª©ì°¨
1. [ë°ì´í„° ì „ì²˜ë¦¬ ë¬¸ì œ](#1-ë°ì´í„°-ì „ì²˜ë¦¬-ë¬¸ì œ)
2. [ëª¨ë¸ í•™ìŠµ ë¬¸ì œ](#2-ëª¨ë¸-í•™ìŠµ-ë¬¸ì œ)
3. [ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”](#3-ë©”ëª¨ë¦¬-ë°-ì„±ëŠ¥-ìµœì í™”)
4. [GPU í™œìš© ë¬¸ì œ](#4-gpu-í™œìš©-ë¬¸ì œ)
5. [ê³¼ì í•© ë° ê³¼ì†Œì í•©](#5-ê³¼ì í•©-ë°-ê³¼ì†Œì í•©)
6. [ëª¨ë¸ ë°°í¬ ë¬¸ì œ](#6-ëª¨ë¸-ë°°í¬-ë¬¸ì œ)
7. [ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë³„ ë¬¸ì œ](#7-ë”¥ëŸ¬ë‹-í”„ë ˆì„ì›Œí¬ë³„-ë¬¸ì œ)
8. [ì‹¤ì „ ì˜ˆì œ ì‹œë‚˜ë¦¬ì˜¤](#8-ì‹¤ì „-ì˜ˆì œ-ì‹œë‚˜ë¦¬ì˜¤)

---

## 1. ë°ì´í„° ì „ì²˜ë¦¬ ë¬¸ì œ

### 1.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬

#### ë¬¸ì œ: ê²°ì¸¡ì¹˜ë¡œ ì¸í•œ í•™ìŠµ ì‹¤íŒ¨

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('data.csv')

# ì—ëŸ¬: ValueError: Input contains NaN
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)  # âŒ NaN ê°’ ë•Œë¬¸ì— ì‹¤íŒ¨
```

**í•´ê²° ë°©ë²•:**

```python
# 1. ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())
print(f"Total missing: {df.isnull().sum().sum()}")

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ í™•ì¸
missing_ratio = df.isnull().sum() / len(df) * 100
print(missing_ratio[missing_ratio > 0])

# 2. ê²°ì¸¡ì¹˜ ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ

# ë°©ë²• 1: í–‰ ì‚­ì œ (ê²°ì¸¡ì¹˜ê°€ ì ì„ ë•Œ)
df_dropped = df.dropna()

# ë°©ë²• 2: ì—´ ì‚­ì œ (íŠ¹ì • ì—´ì— ê²°ì¸¡ì¹˜ê°€ ë§ì„ ë•Œ)
threshold = 0.5  # 50% ì´ìƒ ê²°ì¸¡ì¹˜ë©´ ì‚­ì œ
df_dropped_cols = df.dropna(thresh=len(df) * threshold, axis=1)

# ë°©ë²• 3: í‰ê· /ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
from sklearn.impute import SimpleImputer

# ìˆ˜ì¹˜í˜• ë°ì´í„°
imputer_num = SimpleImputer(strategy='mean')  # ë˜ëŠ” 'median'
df[numeric_cols] = imputer_num.fit_transform(df[numeric_cols])

# ë²”ì£¼í˜• ë°ì´í„°
imputer_cat = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])

# ë°©ë²• 4: ê³ ê¸‰ ëŒ€ì²´ (KNN, Iterative)
from sklearn.impute import KNNImputer, IterativeImputer

# KNN Imputer
knn_imputer = KNNImputer(n_neighbors=5)
df_imputed = pd.DataFrame(
    knn_imputer.fit_transform(df),
    columns=df.columns
)

# Iterative Imputer (MICE)
iter_imputer = IterativeImputer(random_state=42, max_iter=10)
df_imputed = pd.DataFrame(
    iter_imputer.fit_transform(df),
    columns=df.columns
)

# ë°©ë²• 5: ê²°ì¸¡ì¹˜ í‘œì‹œ ë³€ìˆ˜ ì¶”ê°€
for col in df.columns:
    if df[col].isnull().any():
        df[f'{col}_missing'] = df[col].isnull().astype(int)
```

### 1.2 ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬

#### ë¬¸ì œ: í´ë˜ìŠ¤ ë¶ˆê· í˜•ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜

```python
import pandas as pd
from sklearn.datasets import make_classification

# ë¶ˆê· í˜• ë°ì´í„° ìƒì„± (ì˜ˆì‹œ)
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_classes=2,
    weights=[0.95, 0.05],  # 95:5 ë¹„ìœ¨
    random_state=42
)

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
import numpy as np
unique, counts = np.unique(y, return_counts=True)
print(dict(zip(unique, counts)))
# {0: 950, 1: 50}  # ì‹¬í•œ ë¶ˆê· í˜•!
```

**í•´ê²° ë°©ë²•:**

```python
from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.combine import SMOTETomek
from collections import Counter

# 1. Over-sampling

# Random Over-sampling
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
print(f"After ROS: {Counter(y_resampled)}")

# SMOTE (Synthetic Minority Over-sampling Technique)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
print(f"After SMOTE: {Counter(y_resampled)}")

# ADASYN (Adaptive Synthetic)
adasyn = ADASYN(random_state=42)
X_resampled, y_resampled = adasyn.fit_resample(X, y)
print(f"After ADASYN: {Counter(y_resampled)}")

# 2. Under-sampling

# Random Under-sampling
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X, y)
print(f"After RUS: {Counter(y_resampled)}")

# 3. Combination (ê¶Œì¥)
smt = SMOTETomek(random_state=42)
X_resampled, y_resampled = smt.fit_resample(X, y)
print(f"After SMOTETomek: {Counter(y_resampled)}")

# 4. Class Weight ì¡°ì • (ëª¨ë¸ í•™ìŠµ ì‹œ)
from sklearn.ensemble import RandomForestClassifier

# ìë™ ê°€ì¤‘ì¹˜
model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X, y)

# ìˆ˜ë™ ê°€ì¤‘ì¹˜
class_weights = {0: 1, 1: 19}  # í´ë˜ìŠ¤ 1ì— 19ë°° ê°€ì¤‘ì¹˜
model = RandomForestClassifier(class_weight=class_weights, random_state=42)
model.fit(X, y)

# 5. í‰ê°€ ì§€í‘œ ë³€ê²½
from sklearn.metrics import classification_report, roc_auc_score, f1_score

# Accuracy ëŒ€ì‹  ë‹¤ë¥¸ ì§€í‘œ ì‚¬ìš©
predictions = model.predict(X_test)
print(classification_report(y_test, predictions))
print(f"ROC-AUC: {roc_auc_score(y_test, predictions)}")
print(f"F1-Score: {f1_score(y_test, predictions)}")
```

### 1.3 ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©

```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from category_encoders import TargetEncoder, BinaryEncoder

# ì˜ˆì‹œ ë°ì´í„°
df = pd.DataFrame({
    'color': ['red', 'blue', 'green', 'red', 'blue'],
    'size': ['S', 'M', 'L', 'M', 'S'],
    'city': ['Seoul', 'Busan', 'Seoul', 'Incheon', 'Busan'],
    'target': [1, 0, 1, 0, 1]
})

# 1. Label Encoding (ìˆœì„œê°€ ìˆëŠ” ê²½ìš°)
le = LabelEncoder()
df['size_encoded'] = le.fit_transform(df['size'])
print(df[['size', 'size_encoded']])

# 2. One-Hot Encoding (ìˆœì„œê°€ ì—†ëŠ” ê²½ìš°)
# Pandas
df_onehot = pd.get_dummies(df, columns=['color'], prefix='color')

# Scikit-learn
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
color_encoded = ohe.fit_transform(df[['color']])
color_df = pd.DataFrame(
    color_encoded,
    columns=ohe.get_feature_names_out(['color'])
)

# 3. Target Encoding (ì¹´ë””ë„ë¦¬í‹°ê°€ ë†’ì„ ë•Œ)
te = TargetEncoder()
df['city_encoded'] = te.fit_transform(df['city'], df['target'])

# 4. Binary Encoding (ì¹´ë””ë„ë¦¬í‹°ê°€ ë§¤ìš° ë†’ì„ ë•Œ)
be = BinaryEncoder(cols=['city'])
df_binary = be.fit_transform(df)

# 5. Frequency Encoding
freq_encoding = df['city'].value_counts().to_dict()
df['city_freq'] = df['city'].map(freq_encoding)

# 6. ì»¤ìŠ¤í…€ ìˆœì„œ ì¸ì½”ë”©
size_order = {'S': 0, 'M': 1, 'L': 2}
df['size_ordered'] = df['size'].map(size_order)
```

---

## 2. ëª¨ë¸ í•™ìŠµ ë¬¸ì œ

### 2.1 í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ

#### ë¬¸ì œ: Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ê°„ë‹¨í•œ ì‹ ê²½ë§
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Linear(64, 1)
)

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# í•™ìŠµ
for epoch in range(100):
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")
# ì¶œë ¥: Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ! ğŸš¨
```

**ì›ì¸ ë° í•´ê²°:**

**ì›ì¸ 1: í•™ìŠµë¥  ë¬¸ì œ**

```python
# í•™ìŠµë¥ ì´ ë„ˆë¬´ í¼ â†’ Loss ë°œì‚°
optimizer = optim.SGD(model.parameters(), lr=10.0)  # âŒ ë„ˆë¬´ í¼

# í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ìŒ â†’ í•™ìŠµ ì•ˆ ë¨
optimizer = optim.SGD(model.parameters(), lr=1e-10)  # âŒ ë„ˆë¬´ ì‘ìŒ

# í•´ê²°: ì ì ˆí•œ í•™ìŠµë¥  ì°¾ê¸°
# Learning Rate Finder
learning_rates = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
losses = []

for lr in learning_rates:
    model = create_model()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    
    # ëª‡ ë²ˆ í•™ìŠµ
    for _ in range(10):
        loss = train_one_epoch(model, optimizer, train_loader)
    
    losses.append(loss)

# ìµœì  í•™ìŠµë¥  ì„ íƒ
import matplotlib.pyplot as plt
plt.plot(learning_rates, losses)
plt.xscale('log')
plt.xlabel('Learning Rate')
plt.ylabel('Loss')
plt.title('Learning Rate Finder')
plt.show()

# ë˜ëŠ” Learning Rate Scheduler ì‚¬ìš©
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR

# ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

for epoch in range(100):
    train_loss = train(model, train_loader, optimizer)
    val_loss = validate(model, val_loader)
    
    # Scheduler ì—…ë°ì´íŠ¸
    scheduler.step(val_loss)
```

**ì›ì¸ 2: Gradient Vanishing/Exploding**

```python
# Gradient í™•ì¸
def check_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            print(f"{name}: {grad_norm}")

# Gradient Clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# ë˜ëŠ”
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)

# Batch Normalization ì¶”ê°€
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.BatchNorm1d(64),  # âœ… ì¶”ê°€
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.BatchNorm1d(32),  # âœ… ì¶”ê°€
    nn.ReLU(),
    nn.Linear(32, 1)
)

# í™œì„±í™” í•¨ìˆ˜ ë³€ê²½
# Sigmoid/Tanh ëŒ€ì‹  ReLU, LeakyReLU, ELU ì‚¬ìš©
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.LeakyReLU(0.01),  # âœ… LeakyReLU
    nn.Linear(64, 1)
)
```

**ì›ì¸ 3: ë°ì´í„° ì •ê·œí™” ì•ˆ ë¨**

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# ì •ê·œí™” ì „
X_train_raw = train_data

# StandardScaler (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_raw)
X_test_scaled = scaler.transform(X_test_raw)

# MinMaxScaler (0~1 ë²”ìœ„)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_raw)

# ì£¼ì˜: Test ë°ì´í„°ëŠ” Train ë°ì´í„°ì˜ í†µê³„ë¡œ ë³€í™˜!
# âŒ ì˜ëª»ëœ ì˜ˆ
scaler.fit(X_test)  # Trainê³¼ ë‹¤ë¥¸ í†µê³„ ì‚¬ìš©

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ
scaler.fit(X_train)
X_test_scaled = scaler.transform(X_test)
```

### 2.2 ê³¼ì í•© (Overfitting)

#### ì¦ìƒ: Training accuracyëŠ” ë†’ì§€ë§Œ Validation accuracyê°€ ë‚®ìŒ

```python
# í•™ìŠµ ê²°ê³¼
# Epoch 50: Train Loss: 0.05, Train Acc: 0.98
#          Val Loss: 0.45, Val Acc: 0.75  # âŒ í° ì°¨ì´!
```

**í•´ê²° ë°©ë²•:**

```python
import torch.nn as nn
from torch.utils.data import DataLoader

# 1. Dropout ì¶”ê°€
model = nn.Sequential(
    nn.Linear(100, 256),
    nn.ReLU(),
    nn.Dropout(0.5),  # âœ… 50% ë“œë¡­ì•„ì›ƒ
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Dropout(0.3),  # âœ… 30% ë“œë¡­ì•„ì›ƒ
    nn.Linear(128, 10)
)

# 2. L2 Regularization (Weight Decay)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # âœ…

# 3. Early Stopping
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

early_stopping = EarlyStopping(patience=10)

for epoch in range(1000):
    train_loss = train(model, train_loader)
    val_loss = validate(model, val_loader)
    
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print(f"Early stopping at epoch {epoch}")
        break

# 4. ë°ì´í„° ì¦ê°• (Data Augmentation)
from torchvision import transforms

train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])

# 5. ëª¨ë¸ ë³µì¡ë„ ì¤„ì´ê¸°
# Before (ê³¼ë„í•˜ê²Œ ë³µì¡)
model_complex = nn.Sequential(
    nn.Linear(100, 1000),
    nn.ReLU(),
    nn.Linear(1000, 1000),
    nn.ReLU(),
    nn.Linear(1000, 10)
)

# After (ë‹¨ìˆœí™”)
model_simple = nn.Sequential(
    nn.Linear(100, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

# 6. ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘
# ê°€ëŠ¥í•˜ë©´ í•™ìŠµ ë°ì´í„° ì¦ê°€
```

### 2.3 í•™ìŠµ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼

```python
import time
from tqdm import tqdm

# ë¬¸ì œ: í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼
start_time = time.time()
for epoch in range(10):
    for batch in train_loader:
        # í•™ìŠµ ì½”ë“œ
        pass
elapsed = time.time() - start_time
print(f"Training time: {elapsed:.2f}s")  # ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼!
```

**í•´ê²° ë°©ë²•:**

```python
# 1. Batch Size ì¦ê°€
train_loader = DataLoader(
    dataset,
    batch_size=128,  # 32 â†’ 128ë¡œ ì¦ê°€
    shuffle=True,
    num_workers=4  # ë³‘ë ¬ ë°ì´í„° ë¡œë”©
)

# 2. Mixed Precision Training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        
        # Mixed precision
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        
        # Gradient scaling
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

# 3. DataLoader ìµœì í™”
train_loader = DataLoader(
    dataset,
    batch_size=64,
    shuffle=True,
    num_workers=4,      # CPU ë³‘ë ¬ ì²˜ë¦¬
    pin_memory=True,    # GPU ì „ì†¡ ìµœì í™”
    persistent_workers=True  # Worker ì¬ì‚¬ìš©
)

# 4. Gradient Accumulation (ì‘ì€ GPU ë©”ëª¨ë¦¬)
accumulation_steps = 4
optimizer.zero_grad()

for i, (inputs, labels) in enumerate(train_loader):
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss = loss / accumulation_steps  # ì†ì‹¤ ì •ê·œí™”
    
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## 3. ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”

### 3.1 GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

#### ë¬¸ì œ: CUDA out of memory

```python
# ì—ëŸ¬:
# RuntimeError: CUDA out of memory. 
# Tried to allocate 2.00 GiB (GPU 0; 7.79 GiB total capacity)
```

**í•´ê²° ë°©ë²•:**

```python
import torch
import gc

# 1. Batch Size ì¤„ì´ê¸°
train_loader = DataLoader(dataset, batch_size=16)  # 64 â†’ 16

# 2. ë©”ëª¨ë¦¬ ì •ë¦¬
def clear_memory():
    gc.collect()
    torch.cuda.empty_cache()

# í•™ìŠµ ì¤‘ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
for epoch in range(num_epochs):
    train(model, train_loader)
    clear_memory()

# 3. Gradient Checkpointing
from torch.utils.checkpoint import checkpoint

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(1000, 1000)
        self.layer2 = nn.Linear(1000, 1000)
        self.layer3 = nn.Linear(1000, 10)
    
    def forward(self, x):
        # Gradient checkpointing ì‚¬ìš©
        x = checkpoint(self.layer1, x)
        x = checkpoint(self.layer2, x)
        x = self.layer3(x)
        return x

# 4. Mixed Precision Training
from torch.cuda.amp import autocast

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, labels)

# 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
def print_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"Allocated: {allocated:.2f} GB")
        print(f"Reserved: {reserved:.2f} GB")

print_gpu_memory()

# 6. ë¶ˆí•„ìš”í•œ ì—°ì‚° ê·¸ë˜í”„ ì œê±°
with torch.no_grad():
    # í‰ê°€ ì‹œì—ëŠ” gradient ê³„ì‚° ì•ˆ í•¨
    outputs = model(inputs)

# 7. In-place ì—°ì‚° ì‚¬ìš©
x = torch.randn(1000, 1000)
x.relu_()  # in-place
x.add_(5)  # in-place
```

### 3.2 í•™ìŠµ ì†ë„ í”„ë¡œíŒŒì¼ë§

```python
import torch
from torch.profiler import profile, record_function, ProfilerActivity

# ì½”ë“œ í”„ë¡œíŒŒì¼ë§
with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True
) as prof:
    with record_function("model_training"):
        for inputs, labels in train_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

# ê²°ê³¼ ì¶œë ¥
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

# TensorBoardë¡œ ì‹œê°í™”
prof.export_chrome_trace("trace.json")
```

---

## 4. GPU í™œìš© ë¬¸ì œ

### 4.1 GPUê°€ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

```python
import torch

# GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU count: {torch.cuda.device_count()}")
print(f"GPU name: {torch.cuda.get_device_name(0)}")

# ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

model = model.to(device)

# í•™ìŠµ ë£¨í”„
for inputs, labels in train_loader:
    inputs = inputs.to(device)
    labels = labels.to(device)
    
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### 4.2 Multi-GPU í•™ìŠµ

```python
import torch
import torch.nn as nn

# 1. DataParallel (ê°„ë‹¨í•˜ì§€ë§Œ ëŠë¦¼)
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs")
    model = nn.DataParallel(model)

model.to(device)

# 2. DistributedDataParallel (ê¶Œì¥)
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def train_ddp(rank, world_size):
    setup(rank, world_size)
    
    # ëª¨ë¸ ìƒì„±
    model = MyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    
    # í•™ìŠµ
    for epoch in range(num_epochs):
        for inputs, labels in train_loader:
            inputs = inputs.to(rank)
            labels = labels.to(rank)
            
            outputs = ddp_model(inputs)
            loss = criterion(outputs, labels)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    cleanup()

# ì‹¤í–‰
import torch.multiprocessing as mp
world_size = torch.cuda.device_count()
mp.spawn(train_ddp, args=(world_size,), nprocs=world_size, join=True)
```

### 4.3 GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

```bash
# í„°ë¯¸ë„ì—ì„œ GPU ìƒíƒœ í™•ì¸
nvidia-smi

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# Pythonì—ì„œ ëª¨ë‹ˆí„°ë§
```

```python
import torch
import GPUtil

def monitor_gpu():
    """GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    GPUs = GPUtil.getGPUs()
    for gpu in GPUs:
        print(f"GPU {gpu.id}: {gpu.name}")
        print(f"  Temperature: {gpu.temperature}Â°C")
        print(f"  Load: {gpu.load * 100}%")
        print(f"  Memory Used: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB")
        print(f"  Memory Util: {gpu.memoryUtil * 100}%")

monitor_gpu()

# PyTorch GPU ë©”ëª¨ë¦¬
if torch.cuda.is_available():
    print(f"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
    print(f"Max Allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
```

---

## 5. ê³¼ì í•© ë° ê³¼ì†Œì í•©

### 5.1 ê³¼ì†Œì í•© (Underfitting)

#### ì¦ìƒ: Trainê³¼ Validation ì„±ëŠ¥ ëª¨ë‘ ë‚®ìŒ

```python
# ê²°ê³¼:
# Train Acc: 0.65, Val Acc: 0.63  # ë‘˜ ë‹¤ ë‚®ìŒ!
```

**í•´ê²° ë°©ë²•:**

```python
# 1. ëª¨ë¸ ë³µì¡ë„ ì¦ê°€
# Before
model = nn.Sequential(
    nn.Linear(100, 10)  # ë„ˆë¬´ ë‹¨ìˆœ
)

# After
model = nn.Sequential(
    nn.Linear(100, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

# 2. í•™ìŠµ ì‹œê°„ ì¦ê°€
epochs = 100  # 10 â†’ 100

# 3. í•™ìŠµë¥  ì¦ê°€
optimizer = optim.Adam(model.parameters(), lr=0.001)  # 0.0001 â†’ 0.001

# 4. Feature Engineering
# ë” ë§ì€ feature ì¶”ê°€
# ë‹¤í•­ì‹ feature
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# 5. ì •ê·œí™” ê°ì†Œ
# Weight decay ì¤„ì´ê¸°
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)  # 1e-4 â†’ 1e-6

# Dropout ì¤„ì´ê¸°
nn.Dropout(0.2)  # 0.5 â†’ 0.2
```

### 5.2 Learning Curve ë¶„ì„

```python
import matplotlib.pyplot as plt
import numpy as np

def plot_learning_curves(train_losses, val_losses, train_accs, val_accs):
    """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Loss ê³¡ì„ 
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(val_losses, label='Val Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Loss Curves')
    ax1.legend()
    ax1.grid(True)
    
    # Accuracy ê³¡ì„ 
    ax2.plot(train_accs, label='Train Acc')
    ax2.plot(val_accs, label='Val Acc')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Accuracy Curves')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('learning_curves.png', dpi=300)
    plt.show()
    
    # ë¶„ì„
    print("=== Learning Curve Analysis ===")
    
    # ê³¼ì í•© ì²´í¬
    final_train_val_gap = train_accs[-1] - val_accs[-1]
    if final_train_val_gap > 0.1:
        print(f"âš ï¸  Overfitting detected! Gap: {final_train_val_gap:.3f}")
        print("   â†’ Try: Dropout, L2 regularization, Early stopping")
    
    # ê³¼ì†Œì í•© ì²´í¬
    if train_accs[-1] < 0.8 and val_accs[-1] < 0.8:
        print("âš ï¸  Underfitting detected!")
        print("   â†’ Try: Increase model complexity, More epochs")
    
    # ìˆ˜ë ´ í™•ì¸
    if len(val_losses) > 10:
        recent_improvement = np.mean(val_losses[-10:-5]) - np.mean(val_losses[-5:])
        if recent_improvement < 0.001:
            print("âœ“ Model has converged")

# ì‚¬ìš© ì˜ˆì œ
train_losses, val_losses = [], []
train_accs, val_accs = [], []

for epoch in range(num_epochs):
    train_loss, train_acc = train(model, train_loader)
    val_loss, val_acc = validate(model, val_loader)
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

plot_learning_curves(train_losses, val_losses, train_accs, val_accs)
```

---

## 6. ëª¨ë¸ ë°°í¬ ë¬¸ì œ

### 6.1 ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ

```python
import torch

# 1. ì „ì²´ ëª¨ë¸ ì €ì¥ (ë¹„ê¶Œì¥)
torch.save(model, 'model.pth')
loaded_model = torch.load('model.pth')

# 2. State Dict ì €ì¥ (ê¶Œì¥)
torch.save(model.state_dict(), 'model_weights.pth')

# ë¡œë“œ
model = MyModel()
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()

# 3. Checkpoint ì €ì¥ (í•™ìŠµ ì¬ê°œìš©)
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'train_losses': train_losses,
    'val_losses': val_losses
}
torch.save(checkpoint, 'checkpoint.pth')

# Checkpoint ë¡œë“œ
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch']
loss = checkpoint['loss']

# 4. ONNXë¡œ ë³€í™˜ (ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©)
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)

# 5. TorchScriptë¡œ ë³€í™˜ (ìµœì í™”ëœ ë°°í¬)
scripted_model = torch.jit.script(model)
scripted_model.save('model_scripted.pt')

# ë¡œë“œ
loaded_scripted = torch.jit.load('model_scripted.pt')
```

### 6.2 ì¶”ë¡  ìµœì í™”

```python
import torch

# 1. ì¶”ë¡  ëª¨ë“œ
model.eval()

with torch.no_grad():
    outputs = model(inputs)

# 2. TorchScript (JIT ì»´íŒŒì¼)
scripted_model = torch.jit.trace(model, example_input)
scripted_model = torch.jit.script(model)

# 3. ì–‘ìí™” (Quantization)
# Dynamic Quantization
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # ì–‘ìí™”í•  ë ˆì´ì–´
    dtype=torch.qint8
)

# Static Quantization
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
# Calibration
torch.quantization.convert(model, inplace=True)

# 4. ëª¨ë¸ í¬ê¸° ë¹„êµ
def get_model_size(model):
    torch.save(model.state_dict(), "temp.p")
    size = os.path.getsize("temp.p") / 1e6  # MB
    os.remove("temp.p")
    return size

print(f"Original model: {get_model_size(model):.2f} MB")
print(f"Quantized model: {get_model_size(quantized_model):.2f} MB")

# 5. ë°°ì¹˜ ì¶”ë¡ 
def batch_predict(model, data_loader, device):
    """ë°°ì¹˜ë¡œ íš¨ìœ¨ì ì¸ ì¶”ë¡ """
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for batch in data_loader:
            batch = batch.to(device)
            output = model(batch)
            predictions.extend(output.cpu().numpy())
    
    return np.array(predictions)
```

### 6.3 ëª¨ë¸ ì„œë¹™ (FastAPI ì˜ˆì œ)

```python
from fastapi import FastAPI, File, UploadFile
from pydantic import BaseModel
import torch
import numpy as np
from PIL import Image
import io

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
model = torch.load('model.pth')
model.eval()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

class PredictionInput(BaseModel):
    features: list

@app.post("/predict")
async def predict(input_data: PredictionInput):
    """REST API ì˜ˆì¸¡"""
    # ì…ë ¥ ì²˜ë¦¬
    features = torch.tensor([input_data.features], dtype=torch.float32).to(device)
    
    # ì¶”ë¡ 
    with torch.no_grad():
        output = model(features)
        prediction = output.argmax(dim=1).item()
        confidence = torch.softmax(output, dim=1)[0][prediction].item()
    
    return {
        "prediction": int(prediction),
        "confidence": float(confidence)
    }

@app.post("/predict_image")
async def predict_image(file: UploadFile = File(...)):
    """ì´ë¯¸ì§€ ì˜ˆì¸¡"""
    # ì´ë¯¸ì§€ ì½ê¸°
    image_data = await file.read()
    image = Image.open(io.BytesIO(image_data))
    
    # ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    # ì¶”ë¡ 
    with torch.no_grad():
        output = model(image_tensor)
        prediction = output.argmax(dim=1).item()
        confidence = torch.softmax(output, dim=1)[0][prediction].item()
    
    return {
        "prediction": int(prediction),
        "confidence": float(confidence)
    }

# ì‹¤í–‰: uvicorn main:app --reload
```

---

## 7. ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë³„ ë¬¸ì œ

### 7.1 PyTorch íŠ¹ì • ë¬¸ì œ

```python
import torch

# 1. CUDA ì˜¤ë¥˜: "RuntimeError: Expected all tensors to be on the same device"
# ì›ì¸: í…ì„œê°€ ë‹¤ë¥¸ ë””ë°”ì´ìŠ¤ì— ìˆìŒ
model = model.to(device)
inputs = inputs.to(device)  # âœ… ëª¨ë“  í…ì„œë¥¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ

# 2. "RuntimeError: Expected object of backend CPU but got backend CUDA"
# ì›ì¸: CPU í…ì„œì™€ CUDA í…ì„œ í˜¼ìš©
cpu_tensor = torch.randn(10)
cuda_tensor = torch.randn(10).cuda()
result = cpu_tensor + cuda_tensor  # âŒ ì—ëŸ¬!

# í•´ê²°
result = cpu_tensor.cuda() + cuda_tensor  # âœ…

# 3. Inplace ì—°ì‚° ì—ëŸ¬
x = torch.randn(10, requires_grad=True)
x += 1  # âŒ inplace ì—°ì‚°ì€ gradient ê³„ì‚°ì— ë¬¸ì œ
x = x + 1  # âœ…

# 4. DataLoader num_workers ë¬¸ì œ
# Windowsì—ì„œ num_workers > 0ì´ë©´ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥
if __name__ == '__main__':
    train_loader = DataLoader(
        dataset,
        batch_size=32,
        num_workers=4
    )
```

### 7.2 TensorFlow/Keras íŠ¹ì • ë¬¸ì œ

```python
import tensorflow as tf
from tensorflow import keras

# 1. GPU ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì •
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# 2. Mixed Precision
from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# 3. ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ
# ì „ì²´ ëª¨ë¸ ì €ì¥
model.save('model.h5')
model = keras.models.load_model('model.h5')

# Weightsë§Œ ì €ì¥
model.save_weights('weights.h5')
model.load_weights('weights.h5')

# 4. Custom Layer ì§ë ¬í™”
class CustomLayer(keras.layers.Layer):
    def get_config(self):
        config = super().get_config()
        config.update({"my_param": self.my_param})
        return config

# 5. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=10000,
    decay_rate=0.9
)
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)
```

---

## 8. ì‹¤ì „ ì˜ˆì œ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ì „ ê°€ì´ë“œ

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

# 1. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class CustomImageDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        from PIL import Image
        image = Image.open(self.image_paths[idx]).convert('RGB')
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

# 2. ë°ì´í„° ì¦ê°• ë° ì „ì²˜ë¦¬
train_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225])
])

# 3. ë°ì´í„°ì…‹ ìƒì„±
# image_pathsì™€ labelsëŠ” ì‹¤ì œ ë°ì´í„°ë¡œ ëŒ€ì²´
train_paths, val_paths, train_labels, val_labels = train_test_split(
    image_paths, labels, test_size=0.2, random_state=42, stratify=labels
)

train_dataset = CustomImageDataset(train_paths, train_labels, train_transform)
val_dataset = CustomImageDataset(val_paths, val_labels, val_transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, 
                         num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, 
                       num_workers=4, pin_memory=True)

# 4. ëª¨ë¸ ì •ì˜ (ì „ì´ í•™ìŠµ)
model = models.resnet50(pretrained=True)

# ë§ˆì§€ë§‰ ë ˆì´ì–´ êµì²´
num_classes = 10
model.fc = nn.Linear(model.fc.in_features, num_classes)

# 5. ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# 6. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5, verbose=True
)

# 7. í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜
def train_one_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc='Training')
    for inputs, labels in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{100.*correct/total:.2f}%'
        })
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc

def validate(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc='Validation')
        for inputs, labels in pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*correct/total:.2f}%'
            })
    
    epoch_loss = running_loss / len(val_loader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc

# 8. í•™ìŠµ ë£¨í”„
num_epochs = 50
best_val_acc = 0
train_losses, val_losses = [], []
train_accs, val_accs = [], []

for epoch in range(num_epochs):
    print(f'\nEpoch {epoch+1}/{num_epochs}')
    print('-' * 50)
    
    train_loss, train_acc = train_one_epoch(
        model, train_loader, criterion, optimizer, device
    )
    val_loss, val_acc = validate(model, val_loader, criterion, device)
    
    # ê¸°ë¡
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)
    
    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    scheduler.step(val_loss)
    
    # Best ëª¨ë¸ ì €ì¥
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_acc': best_val_acc,
        }, 'best_model.pth')
        print(f'âœ“ Best model saved! (Val Acc: {val_acc:.2f}%)')

# 9. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Curves')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(train_accs, label='Train Acc')
plt.plot(val_accs, label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy Curves')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('training_history.png', dpi=300)
plt.show()

print(f'\nTraining completed!')
print(f'Best validation accuracy: {best_val_acc:.2f}%')
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances

def objective(trial):
    """Optuna objective í•¨ìˆ˜"""
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)
    dropout_rate = trial.suggest_uniform('dropout', 0.1, 0.5)
    hidden_size = trial.suggest_int('hidden_size', 64, 512, step=64)
    
    # ëª¨ë¸ ìƒì„±
    model = nn.Sequential(
        nn.Linear(input_size, hidden_size),
        nn.ReLU(),
        nn.Dropout(dropout_rate),
        nn.Linear(hidden_size, hidden_size // 2),
        nn.ReLU(),
        nn.Dropout(dropout_rate),
        nn.Linear(hidden_size // 2, num_classes)
    ).to(device)
    
    # DataLoader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # Optimizer
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss()
    
    # ì§§ì€ í•™ìŠµ (10 epochs)
    for epoch in range(10):
        model.train()
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
    
    # ê²€ì¦
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    accuracy = correct / total
    
    return accuracy

# Optuna Study ìƒì„±
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# ê²°ê³¼ ì¶œë ¥
print('Best trial:')
trial = study.best_trial
print(f'  Value: {trial.value:.4f}')
print('  Params:')
for key, value in trial.params.items():
    print(f'    {key}: {value}')

# ì‹œê°í™”
plot_optimization_history(study).show()
plot_param_importances(study).show()
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ëª¨ë¸ ì•™ìƒë¸”

```python
import torch
import torch.nn as nn

class EnsembleModel:
    """ì—¬ëŸ¬ ëª¨ë¸ì˜ ì•™ìƒë¸”"""
    
    def __init__(self, models):
        self.models = models
        for model in self.models:
            model.eval()
    
    def predict_average(self, x):
        """í‰ê·  ì•™ìƒë¸”"""
        predictions = []
        with torch.no_grad():
            for model in self.models:
                pred = model(x)
                predictions.append(pred)
        
        return torch.stack(predictions).mean(dim=0)
    
    def predict_voting(self, x):
        """íˆ¬í‘œ ì•™ìƒë¸” (ë¶„ë¥˜)"""
        predictions = []
        with torch.no_grad():
            for model in self.models:
                pred = model(x)
                predictions.append(pred.argmax(dim=1))
        
        predictions = torch.stack(predictions)
        # ë‹¤ìˆ˜ê²° íˆ¬í‘œ
        mode_result = torch.mode(predictions, dim=0)
        return mode_result.values
    
    def predict_weighted(self, x, weights):
        """ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”"""
        predictions = []
        with torch.no_grad():
            for model, weight in zip(self.models, weights):
                pred = model(x)
                predictions.append(pred * weight)
        
        return torch.stack(predictions).sum(dim=0)

# ì‚¬ìš© ì˜ˆì œ
model1 = torch.load('model1.pth')
model2 = torch.load('model2.pth')
model3 = torch.load('model3.pth')

ensemble = EnsembleModel([model1, model2, model3])

# ì˜ˆì¸¡
x = torch.randn(1, 3, 224, 224).to(device)
avg_pred = ensemble.predict_average(x)
voting_pred = ensemble.predict_voting(x)
weighted_pred = ensemble.predict_weighted(x, weights=[0.5, 0.3, 0.2])

print(f"Average prediction: {avg_pred.argmax()}")
print(f"Voting prediction: {voting_pred}")
print(f"Weighted prediction: {weighted_pred.argmax()}")
```

---

## ë¨¸ì‹ ëŸ¬ë‹ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì²´í¬ë¦¬ìŠ¤íŠ¸ âœ…

### ë°ì´í„° ì¤€ë¹„
- [ ] ê²°ì¸¡ì¹˜ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í–ˆëŠ”ê°€?
- [ ] ë°ì´í„°ê°€ ì •ê·œí™”ë˜ì—ˆëŠ”ê°€?
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í–ˆëŠ”ê°€?
- [ ] Train/Val/Testë¥¼ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¦¬í–ˆëŠ”ê°€?
- [ ] ë°ì´í„° ëˆ„ìˆ˜(leakage)ê°€ ì—†ëŠ”ê°€?

### ëª¨ë¸ í•™ìŠµ
- [ ] í•™ìŠµë¥ ì´ ì ì ˆí•œê°€?
- [ ] Lossê°€ ê°ì†Œí•˜ëŠ”ê°€?
- [ ] Gradient vanishing/explodingì´ ì—†ëŠ”ê°€?
- [ ] ë°°ì¹˜ í¬ê¸°ê°€ ì ì ˆí•œê°€?
- [ ] ì¶©ë¶„í•œ epoch ë™ì•ˆ í•™ìŠµí–ˆëŠ”ê°€?

### ì„±ëŠ¥ í‰ê°€
- [ ] ê³¼ì í•©ì´ ë°œìƒí•˜ëŠ”ê°€?
- [ ] ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ í™•ì¸í–ˆëŠ”ê°€?
- [ ] ì ì ˆí•œ í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?
- [ ] Confusion Matrixë¥¼ í™•ì¸í–ˆëŠ”ê°€?

### GPU/ë©”ëª¨ë¦¬
- [ ] GPUê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ”ê°€?
- [ ] ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œê°€ ìˆëŠ”ê°€?
- [ ] Mixed precisionì„ ì‚¬ìš©í•˜ëŠ”ê°€?
- [ ] ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•˜ëŠ”ê°€?

---

## ì¶”ê°€ íŒ ğŸ’¡

### 1. ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´

```python
import torch
import numpy as np
import random

def set_seed(seed=42):
    """ëª¨ë“  ëœë¤ ì‹œë“œ ê³ ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
```

### 2. ì‹¤í—˜ ì¶”ì  (Weights & Biases)

```python
import wandb

# ì´ˆê¸°í™”
wandb.init(project="my-project", name="experiment-1")

# Config ì €ì¥
wandb.config.update({
    "learning_rate": 0.001,
    "epochs": 100,
    "batch_size": 32
})

# í•™ìŠµ ì¤‘ ë¡œê¹…
for epoch in range(num_epochs):
    train_loss, train_acc = train(...)
    val_loss, val_acc = validate(...)
    
    wandb.log({
        "train_loss": train_loss,
        "train_acc": train_acc,
        "val_loss": val_loss,
        "val_acc": val_acc,
        "epoch": epoch
    })

wandb.finish()
```

### 3. ë””ë²„ê¹… ëª¨ë“œ

```python
# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
debug_mode = True

if debug_mode:
    # ë°ì´í„° ì¼ë¶€ë§Œ ì‚¬ìš©
    train_dataset = torch.utils.data.Subset(train_dataset, range(100))
    num_epochs = 2
    print("âš ï¸  Debug mode: Using small dataset")
```

---

## ê²°ë¡ 

ë¨¸ì‹ ëŸ¬ë‹ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì˜ í•µì‹¬:

1. **ë°ì´í„° ìš°ì„ **: ì¢‹ì€ ë°ì´í„°ê°€ ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¦
2. **ë‹¨ê³„ë³„ ê²€ì¦**: ê° ë‹¨ê³„ë¥¼ ê²€ì¦í•˜ë©° ì§„í–‰
3. **ì‹¤í—˜ ê¸°ë¡**: ëª¨ë“  ì‹¤í—˜ì„ ì¶”ì í•˜ê³  ê¸°ë¡
4. **ì¬í˜„ ê°€ëŠ¥ì„±**: ì‹œë“œ ê³ ì •ê³¼ í™˜ê²½ ëª…ì‹œ
5. **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**: í•™ìŠµ ì¤‘ ë©”íŠ¸ë¦­ê³¼ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

Happy Machine Learning! ğŸ¤–
