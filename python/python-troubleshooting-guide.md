# Python íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ğŸ

## ëª©ì°¨
1. [Python í™˜ê²½ ë¬¸ì œ](#1-python-í™˜ê²½-ë¬¸ì œ)
2. [íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì˜ì¡´ì„± ë¬¸ì œ](#2-íŒ¨í‚¤ì§€-ì„¤ì¹˜-ë°-ì˜ì¡´ì„±-ë¬¸ì œ)
3. [Import ì˜¤ë¥˜](#3-import-ì˜¤ë¥˜)
4. [ì¸ì½”ë”© ë¬¸ì œ](#4-ì¸ì½”ë”©-ë¬¸ì œ)
5. [ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ë¬¸ì œ](#5-ë©”ëª¨ë¦¬-ë°-ì„±ëŠ¥-ë¬¸ì œ)
6. [ë©€í‹°í”„ë¡œì„¸ì‹± ë° ë©€í‹°ì“°ë ˆë”©](#6-ë©€í‹°í”„ë¡œì„¸ì‹±-ë°-ë©€í‹°ì“°ë ˆë”©)
7. [ë””ë²„ê¹… ê¸°ë²•](#7-ë””ë²„ê¹…-ê¸°ë²•)
8. [ì‹¤ì „ ì˜ˆì œ ì‹œë‚˜ë¦¬ì˜¤](#8-ì‹¤ì „-ì˜ˆì œ-ì‹œë‚˜ë¦¬ì˜¤)

---

## 1. Python í™˜ê²½ ë¬¸ì œ

### 1.1 ì—¬ëŸ¬ Python ë²„ì „ ì¶©ëŒ

#### ë¬¸ì œ: ì‹œìŠ¤í…œì— ì—¬ëŸ¬ Python ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ í˜¼ë€

```bash
# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ Python í™•ì¸
which python
which python3

# Python ë²„ì „ í™•ì¸
python --version
python3 --version

# ì„¤ì¹˜ëœ ëª¨ë“  Python ì°¾ê¸° (macOS/Linux)
ls -la /usr/bin/python*
ls -la /usr/local/bin/python*

# Windows
where python
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
/usr/bin/python -> python2.7
/usr/bin/python3 -> python3.8
/usr/local/bin/python3 -> python3.11
```

**í•´ê²° ë°©ë²•:**

```bash
# 1. alias ì„¤ì •ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì§€ì •
# ~/.bashrc ë˜ëŠ” ~/.zshrc
alias python=python3.11
alias pip=pip3

# 2. pyenvë¡œ ë²„ì „ ê´€ë¦¬ (ê¶Œì¥)
# pyenv ì„¤ì¹˜
curl https://pyenv.run | bash

# íŠ¹ì • ë²„ì „ ì„¤ì¹˜
pyenv install 3.11.5

# ì „ì—­ Python ë²„ì „ ì„¤ì •
pyenv global 3.11.5

# í”„ë¡œì íŠ¸ë³„ ë²„ì „ ì„¤ì •
cd my-project/
pyenv local 3.11.5

# í™•ì¸
python --version
```

### 1.2 pip vs pip3 ë¬¸ì œ

#### ë¬¸ì œ: pipì™€ pip3ê°€ ë‹¤ë¥¸ Python ë²„ì „ì„ ê°€ë¦¬í‚´

```bash
# pipê°€ ì–´ë–¤ Pythonì„ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
pip --version
# pip 20.0.2 from /usr/lib/python2.7/dist-packages/pip (python 2.7)

pip3 --version
# pip 23.2.1 from /usr/lib/python3.11/site-packages/pip (python 3.11)
```

**í•´ê²° ë°©ë²•:**

```bash
# Python ëª¨ë“ˆë¡œ pip ì‹¤í–‰ (ê¶Œì¥)
python3.11 -m pip install package-name

# ë˜ëŠ” pipë¥¼ ì—…ê·¸ë ˆì´ë“œ
python3 -m pip install --upgrade pip

# íŠ¹ì • ë²„ì „ì˜ pip ì‚¬ìš©
python3.11 -m pip list
```

### 1.3 PATH í™˜ê²½ ë³€ìˆ˜ ë¬¸ì œ

#### ë¬¸ì œ: ì„¤ì¹˜í•œ íŒ¨í‚¤ì§€ì˜ ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ

```bash
# ì—ëŸ¬ ì˜ˆì‹œ:
# command not found: jupyter
# command not found: black

# PATH í™•ì¸
echo $PATH

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ê²½ë¡œ í™•ì¸
python3 -m site --user-site
```

**í•´ê²° ë°©ë²•:**

```bash
# 1. User bin ë””ë ‰í† ë¦¬ë¥¼ PATHì— ì¶”ê°€
# ~/.bashrc ë˜ëŠ” ~/.zshrc
export PATH="$HOME/.local/bin:$PATH"

# macOSì˜ ê²½ìš°
export PATH="$HOME/Library/Python/3.11/bin:$PATH"

# 2. ê°€ìƒí™˜ê²½ ì‚¬ìš© (ê¶Œì¥)
python3 -m venv myenv
source myenv/bin/activate  # Windows: myenv\Scripts\activate

# 3. íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip install --user jupyter
# ë˜ëŠ” ê°€ìƒí™˜ê²½ ë‚´ì—ì„œ
pip install jupyter
```

---

## 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì˜ì¡´ì„± ë¬¸ì œ

### 2.1 íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨

#### ë¬¸ì œ: pip install ì‹¤íŒ¨

```bash
# ì—ëŸ¬ ì˜ˆì‹œ:
pip install numpy

# ERROR: Could not build wheels for numpy, which is required to install 
# pyproject.toml-based projects
```

**ì›ì¸ ë° í•´ê²°:**

**1. ê¶Œí•œ ë¬¸ì œ**

```bash
# ì—ëŸ¬:
# ERROR: Could not install packages due to an OSError: 
# [Errno 13] Permission denied

# í•´ê²° ë°©ë²• 1: --user í”Œë˜ê·¸ ì‚¬ìš©
pip install --user numpy

# í•´ê²° ë°©ë²• 2: ê°€ìƒí™˜ê²½ ì‚¬ìš© (ê¶Œì¥)
python3 -m venv myenv
source myenv/bin/activate
pip install numpy

# í•´ê²° ë°©ë²• 3: sudo ì‚¬ìš© (ë¹„ê¶Œì¥ - ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì˜¤ì—¼ ê°€ëŠ¥)
sudo pip install numpy  # ê¶Œì¥í•˜ì§€ ì•ŠìŒ!
```

**2. ì»´íŒŒì¼ëŸ¬ ëˆ„ë½**

```bash
# macOS
xcode-select --install

# Ubuntu/Debian
sudo apt-get update
sudo apt-get install build-essential python3-dev

# CentOS/RHEL
sudo yum groupinstall "Development Tools"
sudo yum install python3-devel
```

**3. íŠ¹ì • íŒ¨í‚¤ì§€ ë²„ì „ ì¶©ëŒ**

```bash
# ì—ëŸ¬:
# ERROR: pip's dependency resolver does not currently take into account 
# all the packages that are installed

# ì§„ë‹¨
pip check

# í•´ê²°: requirements.txtì— ë²„ì „ ëª…ì‹œ
cat > requirements.txt <<EOF
numpy==1.24.3
pandas==2.0.3
scipy==1.11.1
EOF

pip install -r requirements.txt

# ë˜ëŠ” í˜¸í™˜ ê°€ëŠ¥í•œ ë²„ì „ ì°¾ê¸°
pip install numpy pandas scipy --upgrade
```

### 2.2 ì˜ì¡´ì„± ì§€ì˜¥ (Dependency Hell)

#### ì˜ˆì œ: íŒ¨í‚¤ì§€ AëŠ” numpy>=1.20ì„ ìš”êµ¬, íŒ¨í‚¤ì§€ BëŠ” numpy<1.20ì„ ìš”êµ¬

```bash
# ë¬¸ì œ ìƒí™©
pip install package-a  # requires numpy>=1.20
pip install package-b  # requires numpy<1.20

# ì—ëŸ¬:
# ERROR: Cannot install package-a and package-b because these package 
# versions have conflicting dependencies.
```

**í•´ê²° ë°©ë²•:**

```bash
# 1. ì˜ì¡´ì„± íŠ¸ë¦¬ í™•ì¸
pip install pipdeptree
pipdeptree

# ì¶œë ¥ ì˜ˆì‹œ:
# package-a==1.0.0
#   - numpy [required: >=1.20, installed: 1.24.3]
# package-b==2.0.0
#   - numpy [required: <1.20, installed: 1.24.3]

# 2. ê°€ìƒí™˜ê²½ìœ¼ë¡œ ë¶„ë¦¬
python3 -m venv env_a
source env_a/bin/activate
pip install package-a

deactivate

python3 -m venv env_b
source env_b/bin/activate
pip install package-b

# 3. Docker ì‚¬ìš© (ì™„ì „í•œ ê²©ë¦¬)
# Dockerfile
FROM python:3.11-slim
RUN pip install package-a numpy==1.24.3
```

### 2.3 ìºì‹œ ë¬¸ì œ

#### ë¬¸ì œ: ì´ì „ ì„¤ì¹˜ ì‹¤íŒ¨ë¡œ ì¸í•œ ìºì‹œ ì†ìƒ

```bash
# ì¦ìƒ: íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜ ì‹œ ê³„ì† ì‹¤íŒ¨

# í•´ê²°: pip ìºì‹œ ì‚­ì œ
pip cache purge

# ë˜ëŠ” ìºì‹œ ì—†ì´ ì„¤ì¹˜
pip install --no-cache-dir numpy

# ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
pip cache dir
```

---

## 3. Import ì˜¤ë¥˜

### 3.1 ModuleNotFoundError

#### ë¬¸ì œ: ì„¤ì¹˜í•œ íŒ¨í‚¤ì§€ë¥¼ importí•  ìˆ˜ ì—†ìŒ

```python
# ì—ëŸ¬ ì˜ˆì‹œ
import numpy
# ModuleNotFoundError: No module named 'numpy'
```

**ì§„ë‹¨ ë° í•´ê²°:**

```python
# 1. Pythonì´ íŒ¨í‚¤ì§€ë¥¼ ì°¾ëŠ” ê²½ë¡œ í™•ì¸
import sys
print(sys.path)

# 2. íŒ¨í‚¤ì§€ê°€ ì‹¤ì œë¡œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
import subprocess
result = subprocess.run(['pip', 'list'], capture_output=True, text=True)
print(result.stdout)

# ë˜ëŠ” ì»¤ë§¨ë“œë¼ì¸ì—ì„œ
pip list | grep numpy
```

**ì¼ë°˜ì ì¸ ì›ì¸:**

**ì›ì¸ 1: ë‹¤ë¥¸ Python í™˜ê²½ì— ì„¤ì¹˜**

```bash
# í•´ê²°: í˜„ì¬ Pythonì— ì§ì ‘ ì„¤ì¹˜
python -m pip install numpy

# Jupyterì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
python -m pip install ipykernel
python -m ipykernel install --user --name=myenv
```

**ì›ì¸ 2: ì˜ëª»ëœ ê°€ìƒí™˜ê²½**

```bash
# í˜„ì¬ ê°€ìƒí™˜ê²½ í™•ì¸
which python
echo $VIRTUAL_ENV

# ì˜¬ë°”ë¥¸ ê°€ìƒí™˜ê²½ í™œì„±í™”
source /path/to/correct/venv/bin/activate
```

**ì›ì¸ 3: PYTHONPATH ë¬¸ì œ**

```python
# ì„ì‹œ í•´ê²° (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
import sys
sys.path.append('/path/to/package')

# ì˜êµ¬ í•´ê²°: íŒ¨í‚¤ì§€ë¥¼ editable ëª¨ë“œë¡œ ì„¤ì¹˜
pip install -e /path/to/package
```

### 3.2 ìˆœí™˜ Import (Circular Import)

#### ë¬¸ì œ: ëª¨ë“ˆ ê°„ ìˆœí™˜ ì°¸ì¡°

```python
# module_a.py
from module_b import function_b

def function_a():
    return function_b()

# module_b.py
from module_a import function_a  # âŒ ìˆœí™˜ import!

def function_b():
    return function_a()

# main.py
import module_a  # ImportError: cannot import name 'function_a' from partially initialized module
```

**í•´ê²° ë°©ë²•:**

**ë°©ë²• 1: Import ìœ„ì¹˜ ë³€ê²½**

```python
# module_b.py
def function_b():
    from module_a import function_a  # âœ… í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import
    return function_a()
```

**ë°©ë²• 2: êµ¬ì¡° ì¬ì„¤ê³„**

```python
# common.py - ê³µí†µ ê¸°ëŠ¥ ë¶„ë¦¬
def shared_function():
    pass

# module_a.py
from common import shared_function

def function_a():
    return shared_function()

# module_b.py
from common import shared_function

def function_b():
    return shared_function()
```

**ë°©ë²• 3: Import íƒ€ì… íŒíŒ…ë§Œ ì‚¬ìš©**

```python
# module_a.py
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from module_b import ClassB  # ëŸ°íƒ€ì„ì—ëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ

def function_a(obj: 'ClassB'):  # ë¬¸ìì—´ë¡œ íƒ€ì… íŒíŒ…
    pass
```

### 3.3 ìƒëŒ€ Import ë¬¸ì œ

#### ë¬¸ì œ: ìƒëŒ€ importê°€ ì‘ë™í•˜ì§€ ì•ŠìŒ

```python
# í”„ë¡œì íŠ¸ êµ¬ì¡°:
# myproject/
#   __init__.py
#   module_a.py
#   subpackage/
#     __init__.py
#     module_b.py

# subpackage/module_b.py
from ..module_a import function  # ImportError: attempted relative import with no known parent package
```

**í•´ê²° ë°©ë²•:**

```bash
# ë°©ë²• 1: íŒ¨í‚¤ì§€ë¡œ ì‹¤í–‰
python -m myproject.subpackage.module_b

# ë°©ë²• 2: PYTHONPATH ì„¤ì •
export PYTHONPATH="${PYTHONPATH}:/path/to/myproject"
python subpackage/module_b.py

# ë°©ë²• 3: ì ˆëŒ€ import ì‚¬ìš© (ê¶Œì¥)
# subpackage/module_b.py
from myproject.module_a import function
```

---

## 4. ì¸ì½”ë”© ë¬¸ì œ

### 4.1 íŒŒì¼ ì½ê¸°/ì“°ê¸° ì¸ì½”ë”© ì˜¤ë¥˜

#### ë¬¸ì œ: í•œê¸€ íŒŒì¼ ì²˜ë¦¬ ì‹œ ì˜¤ë¥˜

```python
# ì—ëŸ¬ ì˜ˆì‹œ
with open('data.txt', 'r') as f:
    content = f.read()
# UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc7 in position 0
```

**í•´ê²° ë°©ë²•:**

```python
# 1. ì¸ì½”ë”© ëª…ì‹œ
with open('data.txt', 'r', encoding='utf-8') as f:
    content = f.read()

# 2. ì¸ì½”ë”© ìë™ ê°ì§€
import chardet

with open('data.txt', 'rb') as f:
    raw_data = f.read()
    result = chardet.detect(raw_data)
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

with open('data.txt', 'r', encoding=encoding) as f:
    content = f.read()

# 3. ì—ëŸ¬ ë¬´ì‹œ (ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
with open('data.txt', 'r', encoding='utf-8', errors='ignore') as f:
    content = f.read()

# 4. ì—ëŸ¬ë¥¼ ë‹¤ë¥¸ ë¬¸ìë¡œ ëŒ€ì²´
with open('data.txt', 'r', encoding='utf-8', errors='replace') as f:
    content = f.read()
```

**ì¼ë°˜ì ì¸ ì¸ì½”ë”© ì¢…ë¥˜:**

```python
# ì¸ì½”ë”© ê°ì§€ ë° ë³€í™˜ í•¨ìˆ˜
def read_file_with_encoding(filename):
    """ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ì‹œë„"""
    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1', 'ascii']
    
    for encoding in encodings:
        try:
            with open(filename, 'r', encoding=encoding) as f:
                content = f.read()
            print(f"Successfully read with {encoding}")
            return content
        except UnicodeDecodeError:
            continue
    
    raise ValueError(f"Could not decode {filename} with any encoding")

# ì‚¬ìš©
content = read_file_with_encoding('data.txt')
```

### 4.2 CSV íŒŒì¼ ì¸ì½”ë”© ë¬¸ì œ

```python
import pandas as pd

# ë¬¸ì œ: Excelì—ì„œ ì €ì¥í•œ í•œê¸€ CSV
try:
    df = pd.read_csv('data.csv')
except UnicodeDecodeError as e:
    print(f"Error: {e}")

# í•´ê²° ë°©ë²• 1: ì¸ì½”ë”© ì§€ì •
df = pd.read_csv('data.csv', encoding='cp949')  # Windows í•œê¸€
df = pd.read_csv('data.csv', encoding='euc-kr')  # ë¦¬ëˆ…ìŠ¤ í•œê¸€

# í•´ê²° ë°©ë²• 2: ìë™ ê°ì§€
import chardet

with open('data.csv', 'rb') as f:
    result = chardet.detect(f.read(10000))  # ì²˜ìŒ 10KBë§Œ í™•ì¸
    encoding = result['encoding']

df = pd.read_csv('data.csv', encoding=encoding)

# í•´ê²° ë°©ë²• 3: ì—ëŸ¬ í•¸ë“¤ë§
df = pd.read_csv('data.csv', encoding='utf-8', encoding_errors='ignore')
```

---

## 5. ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ë¬¸ì œ

### 5.1 ë©”ëª¨ë¦¬ ë¶€ì¡± (MemoryError)

#### ë¬¸ì œ: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# ë¬¸ì œ ìƒí™©
import pandas as pd

# 10GB CSV íŒŒì¼ ì½ê¸°
df = pd.read_csv('huge_file.csv')  # MemoryError!
```

**í•´ê²° ë°©ë²•:**

**ë°©ë²• 1: ì²­í¬ë¡œ ì½ê¸°**

```python
import pandas as pd

# ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì½ê¸°
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('huge_file.csv', chunksize=chunk_size):
    # ê° ì²­í¬ ì²˜ë¦¬
    processed = chunk[chunk['value'] > 0]  # í•„í„°ë§ ì˜ˆì‹œ
    chunks.append(processed)

# í•„ìš”ì‹œ ê²°í•©
df = pd.concat(chunks, ignore_index=True)

# ë˜ëŠ” ì²­í¬ë³„ë¡œ ì²˜ë¦¬ í›„ ì €ì¥
for i, chunk in enumerate(pd.read_csv('huge_file.csv', chunksize=chunk_size)):
    processed = process_chunk(chunk)
    processed.to_csv(f'output_{i}.csv', index=False)
```

**ë°©ë²• 2: ë°ì´í„° íƒ€ì… ìµœì í™”**

```python
import pandas as pd
import numpy as np

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
df = pd.read_csv('data.csv')
print(df.memory_usage(deep=True))
print(f"Total memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# ë°ì´í„° íƒ€ì… ìµœì í™”
def optimize_dtypes(df):
    """ë°ì´í„°í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    
    # ì •ìˆ˜í˜• ìµœì í™”
    for col in df.select_dtypes(include=['int']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    
    # ì‹¤ìˆ˜í˜• ìµœì í™”
    for col in df.select_dtypes(include=['float']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')
    
    # ê°ì²´í˜•ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜ (ë°˜ë³µë˜ëŠ” ê°’ì´ ë§ì€ ê²½ìš°)
    for col in df.select_dtypes(include=['object']).columns:
        num_unique = df[col].nunique()
        num_total = len(df[col])
        if num_unique / num_total < 0.5:  # 50% ì´í•˜ë©´ ì¹´í…Œê³ ë¦¬ë¡œ
            df[col] = df[col].astype('category')
    
    return df

df_optimized = optimize_dtypes(df)
print(f"Optimized memory: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

**ë°©ë²• 3: í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸°**

```python
# íŠ¹ì • ì»¬ëŸ¼ë§Œ ì„ íƒ
df = pd.read_csv('huge_file.csv', usecols=['col1', 'col2', 'col3'])

# íŠ¹ì • í–‰ë§Œ ì½ê¸°
df = pd.read_csv('huge_file.csv', nrows=100000)

# ì¡°ê±´ì— ë§ëŠ” í–‰ë§Œ ì½ê¸° (ë¶ˆê°€ëŠ¥ - ëŒ€ì‹  ì²­í¬ ì‚¬ìš©)
chunks = []
for chunk in pd.read_csv('huge_file.csv', chunksize=10000):
    filtered = chunk[chunk['date'] > '2023-01-01']
    chunks.append(filtered)
df = pd.concat(chunks)
```

**ë°©ë²• 4: Dask ì‚¬ìš© (ë³‘ë ¬ ì²˜ë¦¬)**

```python
import dask.dataframe as dd

# Daskë¡œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
df = dd.read_csv('huge_file.csv')

# Pandasì™€ ìœ ì‚¬í•œ API
result = df[df['value'] > 0].groupby('category').mean()

# ê³„ì‚° ì‹¤í–‰
result = result.compute()  # ì´ ì‹œì ì— ì‹¤ì œ ê³„ì‚° ìˆ˜í–‰
```

### 5.2 ëŠë¦° ì½”ë“œ ìµœì í™”

#### ë¬¸ì œ: ë°˜ë³µë¬¸ì´ ë„ˆë¬´ ëŠë¦¼

```python
import time

# ëŠë¦° ì½”ë“œ
start = time.time()
result = []
for i in range(1000000):
    result.append(i ** 2)
print(f"Time: {time.time() - start:.2f}s")  # ~0.15s
```

**ìµœì í™” ë°©ë²•:**

**ë°©ë²• 1: List Comprehension**

```python
# ë” ë¹ ë¥¸ ì½”ë“œ
start = time.time()
result = [i ** 2 for i in range(1000000)]
print(f"Time: {time.time() - start:.2f}s")  # ~0.09s
```

**ë°©ë²• 2: NumPy ë²¡í„°í™”**

```python
import numpy as np

# ê°€ì¥ ë¹ ë¥¸ ì½”ë“œ
start = time.time()
result = np.arange(1000000) ** 2
print(f"Time: {time.time() - start:.2f}s")  # ~0.003s
```

**ë°©ë²• 3: Numba JIT ì»´íŒŒì¼**

```python
from numba import jit
import numpy as np

@jit(nopython=True)
def compute_squares(n):
    result = np.empty(n, dtype=np.int64)
    for i in range(n):
        result[i] = i ** 2
    return result

start = time.time()
result = compute_squares(1000000)
print(f"Time: {time.time() - start:.2f}s")  # ~0.002s (ì²« ì‹¤í–‰ í›„)
```

**ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§:**

```python
# cProfileë¡œ ë³‘ëª© ì§€ì  ì°¾ê¸°
import cProfile
import pstats

def slow_function():
    total = 0
    for i in range(1000000):
        total += i ** 2
    return total

# í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
profiler = cProfile.Profile()
profiler.enable()
result = slow_function()
profiler.disable()

# ê²°ê³¼ ì¶œë ¥
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(10)  # ìƒìœ„ 10ê°œ í•¨ìˆ˜

# ë˜ëŠ” ê°„ë‹¨í•˜ê²Œ
cProfile.run('slow_function()', sort='cumulative')
```

**line_profilerë¡œ ë¼ì¸ë³„ ë¶„ì„:**

```bash
# ì„¤ì¹˜
pip install line_profiler

# ì½”ë“œì— @profile ë°ì½”ë ˆì´í„° ì¶”ê°€
# script.py
@profile
def slow_function():
    result = []
    for i in range(100000):
        result.append(i ** 2)
    return result

# ì‹¤í–‰
kernprof -l -v script.py
```

---

## 6. ë©€í‹°í”„ë¡œì„¸ì‹± ë° ë©€í‹°ì“°ë ˆë”©

### 6.1 GIL (Global Interpreter Lock) ë¬¸ì œ

#### ë¬¸ì œ: ë©€í‹°ì“°ë ˆë”©ì´ ì˜ˆìƒë³´ë‹¤ ëŠë¦¼

```python
import threading
import time

def cpu_bound_task(n):
    """CPU ì§‘ì•½ì  ì‘ì—…"""
    total = 0
    for i in range(n):
        total += i ** 2
    return total

# ë‹¨ì¼ ì“°ë ˆë“œ
start = time.time()
result1 = cpu_bound_task(10000000)
result2 = cpu_bound_task(10000000)
print(f"Sequential: {time.time() - start:.2f}s")  # ~2.0s

# ë©€í‹°ì“°ë ˆë”© (GIL ë•Œë¬¸ì— ëŠë¦¼!)
start = time.time()
thread1 = threading.Thread(target=cpu_bound_task, args=(10000000,))
thread2 = threading.Thread(target=cpu_bound_task, args=(10000000,))
thread1.start()
thread2.start()
thread1.join()
thread2.join()
print(f"Threading: {time.time() - start:.2f}s")  # ~2.1s (ë” ëŠë¦´ ìˆ˜ë„!)
```

**í•´ê²° ë°©ë²•: ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©**

```python
from multiprocessing import Pool
import time

def cpu_bound_task(n):
    total = 0
    for i in range(n):
        total += i ** 2
    return total

if __name__ == '__main__':
    # ë©€í‹°í”„ë¡œì„¸ì‹±
    start = time.time()
    with Pool(processes=2) as pool:
        results = pool.map(cpu_bound_task, [10000000, 10000000])
    print(f"Multiprocessing: {time.time() - start:.2f}s")  # ~1.1s (ì‹¤ì œë¡œ ë¹ ë¦„!)
```

### 6.2 ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë¥˜

#### ë¬¸ì œ: pickle ì˜¤ë¥˜

```python
from multiprocessing import Pool

class MyClass:
    def __init__(self, value):
        self.value = value
    
    def process(self, x):
        return x * self.value

# ì—ëŸ¬ ë°œìƒ!
obj = MyClass(10)
with Pool() as pool:
    result = pool.map(obj.process, range(10))
# AttributeError: Can't pickle local object
```

**í•´ê²° ë°©ë²•:**

```python
# ë°©ë²• 1: ì „ì—­ í•¨ìˆ˜ ì‚¬ìš©
def process_global(args):
    x, value = args
    return x * value

with Pool() as pool:
    result = pool.starmap(process_global, [(i, 10) for i in range(10)])

# ë°©ë²• 2: __call__ ë©”ì„œë“œ ì‚¬ìš©
class MyClass:
    def __init__(self, value):
        self.value = value
    
    def __call__(self, x):
        return x * self.value

# ë°©ë²• 3: pathos ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
from pathos.multiprocessing import ProcessPool

class MyClass:
    def __init__(self, value):
        self.value = value
    
    def process(self, x):
        return x * self.value

obj = MyClass(10)
with ProcessPool() as pool:
    result = pool.map(obj.process, range(10))
```

### 6.3 ê³µìœ  ë©”ëª¨ë¦¬ ë¬¸ì œ

```python
from multiprocessing import Pool, Manager

# ë¬¸ì œ: ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©
counter = 0

def increment(x):
    global counter
    counter += 1  # ì´ ë³€ê²½ì‚¬í•­ì€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ!
    return x ** 2

with Pool(4) as pool:
    results = pool.map(increment, range(100))

print(f"Counter: {counter}")  # 0 (ë³€ê²½ë˜ì§€ ì•ŠìŒ!)
```

**í•´ê²° ë°©ë²•:**

```python
from multiprocessing import Pool, Manager, Value, Lock

# ë°©ë²• 1: Manager ì‚¬ìš©
def increment_with_manager(args):
    x, shared_dict = args
    shared_dict['counter'] += 1
    return x ** 2

if __name__ == '__main__':
    with Manager() as manager:
        shared_dict = manager.dict({'counter': 0})
        args = [(i, shared_dict) for i in range(100)]
        
        with Pool(4) as pool:
            results = pool.map(increment_with_manager, args)
        
        print(f"Counter: {shared_dict['counter']}")  # 100

# ë°©ë²• 2: Valueì™€ Lock ì‚¬ìš© (ë” ë¹ ë¦„)
def increment_with_value(args):
    x, counter, lock = args
    with lock:
        counter.value += 1
    return x ** 2

if __name__ == '__main__':
    counter = Value('i', 0)  # ì •ìˆ˜ íƒ€ì… ê³µìœ  ë³€ìˆ˜
    lock = Lock()
    
    args = [(i, counter, lock) for i in range(100)]
    
    with Pool(4) as pool:
        results = pool.map(increment_with_value, args)
    
    print(f"Counter: {counter.value}")  # 100
```

---

## 7. ë””ë²„ê¹… ê¸°ë²•

### 7.1 ê¸°ë³¸ ë””ë²„ê¹…

```python
# 1. print() ë””ë²„ê¹…
def complex_function(data):
    print(f"[DEBUG] Input: {data}")
    result = process_data(data)
    print(f"[DEBUG] Result: {result}")
    return result

# 2. logging ì‚¬ìš© (ê¶Œì¥)
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def complex_function(data):
    logger.debug(f"Input: {data}")
    result = process_data(data)
    logger.info(f"Result: {result}")
    return result

# 3. assert ì‚¬ìš©
def divide(a, b):
    assert b != 0, "Divisor cannot be zero"
    assert isinstance(a, (int, float)), "a must be numeric"
    assert isinstance(b, (int, float)), "b must be numeric"
    return a / b
```

### 7.2 pdb ë””ë²„ê±° ì‚¬ìš©

```python
import pdb

def buggy_function(data):
    result = []
    for item in data:
        # ë””ë²„ê±° ì‹œì‘ì 
        pdb.set_trace()  # ë˜ëŠ” breakpoint() (Python 3.7+)
        
        processed = item * 2
        result.append(processed)
    return result

# ì‹¤í–‰ ì‹œ ëŒ€í™”í˜• ë””ë²„ê±° ì‹¤í–‰
# ì£¼ìš” ëª…ë ¹ì–´:
# n (next): ë‹¤ìŒ ì¤„
# s (step): í•¨ìˆ˜ ë‚´ë¶€ë¡œ ë“¤ì–´ê°€ê¸°
# c (continue): ë‹¤ìŒ breakpointê¹Œì§€ ê³„ì†
# l (list): í˜„ì¬ ì½”ë“œ ë³´ê¸°
# p variable: ë³€ìˆ˜ ì¶œë ¥
# q (quit): ì¢…ë£Œ
```

### 7.3 ì˜ˆì™¸ ì²˜ë¦¬ ë° íŠ¸ë ˆì´ìŠ¤ë°±

```python
import traceback
import sys

def risky_function():
    try:
        result = 1 / 0
    except ZeroDivisionError as e:
        # ë°©ë²• 1: ê¸°ë³¸ ì—ëŸ¬ ë©”ì‹œì§€
        print(f"Error: {e}")
        
        # ë°©ë²• 2: ì „ì²´ íŠ¸ë ˆì´ìŠ¤ë°± ì¶œë ¥
        traceback.print_exc()
        
        # ë°©ë²• 3: íŠ¸ë ˆì´ìŠ¤ë°±ì„ ë¬¸ìì—´ë¡œ ì €ì¥
        tb_str = traceback.format_exc()
        print(tb_str)
        
        # ë°©ë²• 4: ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        exc_type, exc_value, exc_traceback = sys.exc_info()
        print(f"Type: {exc_type}")
        print(f"Value: {exc_value}")
        
        # ë°©ë²• 5: ë¡œê·¸ì— ê¸°ë¡
        import logging
        logging.exception("Exception occurred")

# ì»¤ìŠ¤í…€ ì˜ˆì™¸
class DataValidationError(Exception):
    """ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, field, value, message="Invalid data"):
        self.field = field
        self.value = value
        self.message = f"{message}: {field}={value}"
        super().__init__(self.message)

def validate_data(data):
    if 'age' not in data:
        raise DataValidationError('age', None, 'Missing required field')
    if data['age'] < 0:
        raise DataValidationError('age', data['age'], 'Age cannot be negative')
```

---

## 8. ì‹¤ì „ ì˜ˆì œ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ ì²˜ë¦¬

**ë¬¸ì œ:** 10GB CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
import pandas as pd
import numpy as np
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LargeCSVProcessor:
    """ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, input_file, output_file, chunk_size=100000):
        self.input_file = input_file
        self.output_file = output_file
        self.chunk_size = chunk_size
    
    def process_chunk(self, chunk):
        """ê°œë³„ ì²­í¬ ì²˜ë¦¬ ë¡œì§"""
        # ì˜ˆ: íŠ¹ì • ì¡°ê±´ í•„í„°ë§
        chunk = chunk[chunk['value'] > 0]
        
        # ë°ì´í„° íƒ€ì… ìµœì í™”
        chunk = self.optimize_dtypes(chunk)
        
        # ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€
        chunk['processed_value'] = chunk['value'] * 2
        
        return chunk
    
    def optimize_dtypes(self, df):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        for col in df.select_dtypes(include=['int']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')
        
        for col in df.select_dtypes(include=['float']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
        
        return df
    
    def process(self):
        """ì „ì²´ íŒŒì¼ ì²˜ë¦¬"""
        logger.info(f"Processing {self.input_file}")
        
        # ì²« ë²ˆì§¸ ì²­í¬ë¡œ í—¤ë” ìƒì„±
        first_chunk = True
        total_rows = 0
        
        try:
            for i, chunk in enumerate(pd.read_csv(
                self.input_file, 
                chunksize=self.chunk_size,
                low_memory=False
            )):
                logger.info(f"Processing chunk {i+1} ({len(chunk)} rows)")
                
                # ì²­í¬ ì²˜ë¦¬
                processed = self.process_chunk(chunk)
                
                # ê²°ê³¼ ì €ì¥
                processed.to_csv(
                    self.output_file,
                    mode='w' if first_chunk else 'a',
                    header=first_chunk,
                    index=False
                )
                
                first_chunk = False
                total_rows += len(processed)
                
                logger.info(f"Total processed: {total_rows} rows")
        
        except Exception as e:
            logger.error(f"Error processing file: {e}")
            raise
        
        logger.info(f"Processing complete. Total rows: {total_rows}")
        return total_rows

# ì‚¬ìš© ì˜ˆì œ
if __name__ == '__main__':
    processor = LargeCSVProcessor(
        input_file='huge_data.csv',
        output_file='processed_data.csv',
        chunk_size=100000
    )
    
    total = processor.process()
    print(f"Processed {total} rows successfully")
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ

**ë¬¸ì œ:** ìˆ˜ë°±ë§Œ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ë° ë„ˆë¬´ ëŠë¦¼

```python
from multiprocessing import Pool, cpu_count
from pathlib import Path
from PIL import Image
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImageProcessor:
    """ì´ë¯¸ì§€ ë³‘ë ¬ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, input_dir, output_dir, target_size=(224, 224)):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.target_size = target_size
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def process_single_image(args):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì •ì  ë©”ì„œë“œ - pickle ê°€ëŠ¥)"""
        input_path, output_path, target_size = args
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            img = Image.open(input_path)
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            img_resized = img.resize(target_size, Image.LANCZOS)
            
            # ì €ì¥
            img_resized.save(output_path, quality=95)
            
            return True, input_path.name
        
        except Exception as e:
            logger.error(f"Error processing {input_path}: {e}")
            return False, input_path.name
    
    def get_image_files(self):
        """ì²˜ë¦¬í•  ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
        return [
            f for f in self.input_dir.iterdir() 
            if f.suffix.lower() in extensions
        ]
    
    def process_sequential(self):
        """ìˆœì°¨ ì²˜ë¦¬ (ë¹„êµìš©)"""
        logger.info("Starting sequential processing...")
        start_time = time.time()
        
        files = self.get_image_files()
        results = []
        
        for input_path in files:
            output_path = self.output_dir / input_path.name
            result = self.process_single_image(
                (input_path, output_path, self.target_size)
            )
            results.append(result)
        
        elapsed = time.time() - start_time
        success_count = sum(1 for r, _ in results if r)
        
        logger.info(f"Sequential: {success_count}/{len(files)} images in {elapsed:.2f}s")
        return results
    
    def process_parallel(self, num_workers=None):
        """ë³‘ë ¬ ì²˜ë¦¬"""
        if num_workers is None:
            num_workers = cpu_count()
        
        logger.info(f"Starting parallel processing with {num_workers} workers...")
        start_time = time.time()
        
        # ì‘ì—… ëª©ë¡ ìƒì„±
        files = self.get_image_files()
        tasks = [
            (f, self.output_dir / f.name, self.target_size)
            for f in files
        ]
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with Pool(processes=num_workers) as pool:
            results = pool.map(self.process_single_image, tasks)
        
        elapsed = time.time() - start_time
        success_count = sum(1 for r, _ in results if r)
        
        logger.info(f"Parallel: {success_count}/{len(files)} images in {elapsed:.2f}s")
        logger.info(f"Speedup: {len(files)/elapsed:.2f} images/sec")
        
        return results

# ì‚¬ìš© ì˜ˆì œ
if __name__ == '__main__':
    processor = ImageProcessor(
        input_dir='./input_images',
        output_dir='./output_images',
        target_size=(224, 224)
    )
    
    # ìˆœì°¨ ì²˜ë¦¬
    processor.process_sequential()
    
    # ë³‘ë ¬ ì²˜ë¦¬
    processor.process_parallel(num_workers=4)
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë³µì¡í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…

**ë¬¸ì œ:** ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹˜ëŠ” ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì–´ë””ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ”ì§€ ì°¾ê¸° ì–´ë ¤ì›€

```python
import pandas as pd
import logging
from functools import wraps
import time
from typing import Callable, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def log_step(func: Callable) -> Callable:
    """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¥¼ ë¡œê¹…í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        step_name = func.__name__
        logger.info(f"Starting step: {step_name}")
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            elapsed = time.time() - start_time
            
            # ê²°ê³¼ ì •ë³´ ë¡œê¹…
            if isinstance(result, pd.DataFrame):
                logger.info(f"Step {step_name} completed in {elapsed:.2f}s")
                logger.info(f"  Output shape: {result.shape}")
                logger.info(f"  Memory usage: {result.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
            else:
                logger.info(f"Step {step_name} completed in {elapsed:.2f}s")
            
            return result
        
        except Exception as e:
            logger.error(f"Step {step_name} failed: {str(e)}")
            logger.exception("Full traceback:")
            raise
    
    return wrapper

class DataPipeline:
    """ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, data):
        self.data = data
        self.history = []
    
    @log_step
    def load_data(self, filepath):
        """ë°ì´í„° ë¡œë“œ"""
        logger.info(f"Loading data from {filepath}")
        self.data = pd.read_csv(filepath)
        self.history.append(('load', self.data.shape))
        return self
    
    @log_step
    def clean_data(self):
        """ë°ì´í„° ì •ì œ"""
        logger.info("Cleaning data...")
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        missing = self.data.isnull().sum()
        if missing.any():
            logger.warning(f"Missing values found:\n{missing[missing > 0]}")
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        self.data = self.data.dropna()
        
        # ì¤‘ë³µ ì œê±°
        duplicates = self.data.duplicated().sum()
        if duplicates > 0:
            logger.warning(f"Found {duplicates} duplicate rows")
            self.data = self.data.drop_duplicates()
        
        self.history.append(('clean', self.data.shape))
        return self
    
    @log_step
    def validate_data(self):
        """ë°ì´í„° ê²€ì¦"""
        logger.info("Validating data...")
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        expected_types = {
            'id': 'int64',
            'name': 'object',
            'value': 'float64'
        }
        
        for col, expected_type in expected_types.items():
            if col in self.data.columns:
                actual_type = str(self.data[col].dtype)
                if actual_type != expected_type:
                    logger.warning(
                        f"Column {col} has type {actual_type}, expected {expected_type}"
                    )
        
        # ë²”ìœ„ ê²€ì¦
        if 'value' in self.data.columns:
            invalid = self.data[self.data['value'] < 0]
            if not invalid.empty:
                logger.error(f"Found {len(invalid)} rows with negative values")
                raise ValueError("Negative values not allowed in 'value' column")
        
        self.history.append(('validate', self.data.shape))
        return self
    
    @log_step
    def transform_data(self):
        """ë°ì´í„° ë³€í™˜"""
        logger.info("Transforming data...")
        
        # ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€
        if 'value' in self.data.columns:
            self.data['value_squared'] = self.data['value'] ** 2
            self.data['value_log'] = self.data['value'].apply(
                lambda x: pd.np.log1p(x) if x > 0 else 0
            )
        
        self.history.append(('transform', self.data.shape))
        return self
    
    @log_step
    def aggregate_data(self, group_by):
        """ë°ì´í„° ì§‘ê³„"""
        logger.info(f"Aggregating data by {group_by}...")
        
        if group_by not in self.data.columns:
            raise ValueError(f"Column {group_by} not found in data")
        
        self.data = self.data.groupby(group_by).agg({
            'value': ['mean', 'sum', 'count'],
            'value_squared': 'mean'
        }).reset_index()
        
        self.history.append(('aggregate', self.data.shape))
        return self
    
    @log_step
    def save_data(self, filepath):
        """ë°ì´í„° ì €ì¥"""
        logger.info(f"Saving data to {filepath}")
        self.data.to_csv(filepath, index=False)
        self.history.append(('save', self.data.shape))
        return self
    
    def print_history(self):
        """íŒŒì´í”„ë¼ì¸ íˆìŠ¤í† ë¦¬ ì¶œë ¥"""
        logger.info("Pipeline execution history:")
        for step, shape in self.history:
            logger.info(f"  {step}: {shape}")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == '__main__':
    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = DataPipeline(data=None)
        pipeline \
            .load_data('input.csv') \
            .clean_data() \
            .validate_data() \
            .transform_data() \
            .aggregate_data('category') \
            .save_data('output.csv')
        
        # íˆìŠ¤í† ë¦¬ ì¶œë ¥
        pipeline.print_history()
        
        logger.info("Pipeline completed successfully!")
    
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise
```

### ì‹œë‚˜ë¦¬ì˜¤ 4: ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

**ë¬¸ì œ:** í”„ë¡œê·¸ë¨ì´ ë©”ëª¨ë¦¬ë¥¼ ë„ˆë¬´ ë§ì´ ì‚¬ìš©í•¨

```python
from memory_profiler import profile
import numpy as np

@profile
def memory_intensive_function():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ í•¨ìˆ˜"""
    # í° ë°°ì—´ ìƒì„±
    large_array = np.random.rand(10000, 10000)  # ~800MB
    
    # ì²˜ë¦¬
    result = large_array ** 2
    
    # ë©”ëª¨ë¦¬ í•´ì œë¥¼ ìœ„í•´ del ì‚¬ìš©
    del large_array
    
    return result

# ì‹¤í–‰
# python -m memory_profiler script.py

# ê²°ê³¼:
# Line #    Mem usage    Increment  Occurrences   Line Contents
# ============================================================
#      5   45.7 MiB   45.7 MiB           1   @profile
#      6                                     def memory_intensive_function():
#      8  845.7 MiB  800.0 MiB           1       large_array = np.random.rand(10000, 10000)
#     11 1645.7 MiB  800.0 MiB           1       result = large_array ** 2
#     14  845.7 MiB -800.0 MiB           1       del large_array
#     16  845.7 MiB    0.0 MiB           1       return result
```

**ë©”ëª¨ë¦¬ ìµœì í™” ì˜ˆì œ:**

```python
import numpy as np
from memory_profiler import profile

@profile
def optimized_function():
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë²„ì „"""
    # ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
    chunk_size = 1000
    total_size = 10000
    
    results = []
    for i in range(0, total_size, chunk_size):
        chunk = np.random.rand(chunk_size, total_size)
        chunk_result = chunk ** 2
        results.append(chunk_result)
        del chunk  # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
    
    # ê²°ê³¼ í•©ì¹˜ê¸°
    final_result = np.vstack(results)
    return final_result

# ë˜ëŠ” ì œë„ˆë ˆì´í„° ì‚¬ìš©
def generate_chunks(size, chunk_size):
    """ì²­í¬ë¥¼ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„°"""
    for i in range(0, size, chunk_size):
        chunk = np.random.rand(chunk_size, size)
        yield chunk ** 2

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë”ìš± ì¤„ì„
result_iterator = generate_chunks(10000, 1000)
```

---

## ìœ ìš©í•œ Python ë„êµ¬ ëª¨ìŒ

### ë””ë²„ê¹… ë„êµ¬

```bash
# pdb - ê¸°ë³¸ ë””ë²„ê±°
python -m pdb script.py

# ipdb - IPython í†µí•© ë””ë²„ê±°
pip install ipdb
python -m ipdb script.py

# pudb - ì‹œê°ì  ë””ë²„ê±°
pip install pudb
python -m pudb script.py
```

### í”„ë¡œíŒŒì¼ë§ ë„êµ¬

```bash
# cProfile - CPU í”„ë¡œíŒŒì¼ë§
python -m cProfile -s cumulative script.py

# line_profiler - ë¼ì¸ë³„ í”„ë¡œíŒŒì¼ë§
pip install line_profiler
kernprof -l -v script.py

# memory_profiler - ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
pip install memory_profiler
python -m memory_profiler script.py

# py-spy - ì‹¤ì‹œê°„ í”„ë¡œíŒŒì¼ë§
pip install py-spy
py-spy top -- python script.py
```

### ì½”ë“œ í’ˆì§ˆ ë„êµ¬

```bash
# pylint - ì •ì  ë¶„ì„
pip install pylint
pylint script.py

# flake8 - ìŠ¤íƒ€ì¼ ê²€ì‚¬
pip install flake8
flake8 script.py

# black - ìë™ í¬ë§¤íŒ…
pip install black
black script.py

# mypy - íƒ€ì… ì²´í‚¹
pip install mypy
mypy script.py
```

---

## Python íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì²´í¬ë¦¬ìŠ¤íŠ¸ âœ…

### í™˜ê²½ ë¬¸ì œ
- [ ] Python ë²„ì „ì´ ì˜¬ë°”ë¥¸ê°€?
- [ ] ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì—ˆëŠ”ê°€?
- [ ] PATHê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ê°€?
- [ ] pipê°€ ì˜¬ë°”ë¥¸ Pythonì„ ê°€ë¦¬í‚¤ëŠ”ê°€?

### íŒ¨í‚¤ì§€ ë¬¸ì œ
- [ ] íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ê°€?
- [ ] ë²„ì „ ì¶©ëŒì´ ì—†ëŠ”ê°€?
- [ ] requirements.txtê°€ ìµœì‹ ì¸ê°€?
- [ ] ê°€ìƒí™˜ê²½ì„ ì‚¬ìš©í•˜ëŠ”ê°€?

### Import ë¬¸ì œ
- [ ] ëª¨ë“ˆ ì´ë¦„ì´ ì •í™•í•œê°€?
- [ ] PYTHONPATHê°€ ì˜¬ë°”ë¥¸ê°€?
- [ ] ìˆœí™˜ importê°€ ì—†ëŠ”ê°€?
- [ ] __init__.pyê°€ ìˆëŠ”ê°€?

### ì„±ëŠ¥ ë¬¸ì œ
- [ ] ë¶ˆí•„ìš”í•œ ë°˜ë³µë¬¸ì´ ìˆëŠ”ê°€?
- [ ] NumPy ë²¡í„°í™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ê°€?
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ì ˆí•œê°€?
- [ ] ë³‘ë ¬ ì²˜ë¦¬ê°€ í•„ìš”í•œê°€?

---

## ì¶”ê°€ íŒ ğŸ’¡

### 1. ê°€ìƒí™˜ê²½ ê´€ë¦¬ Best Practices

```bash
# í”„ë¡œì íŠ¸ë³„ ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv venv

# í™œì„±í™”
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows

# requirements.txt ìƒì„±
pip freeze > requirements.txt

# requirements.txtì—ì„œ ì„¤ì¹˜
pip install -r requirements.txt

# ê°€ìƒí™˜ê²½ ë¹„í™œì„±í™”
deactivate
```

### 2. ë””ë²„ê¹… íŒ

```python
# 1. print ëŒ€ì‹  pprint ì‚¬ìš©
from pprint import pprint
pprint(complex_dict)

# 2. breakpoint() í™œìš© (Python 3.7+)
def function():
    x = 10
    breakpoint()  # ì—¬ê¸°ì„œ ë””ë²„ê±° ì‹œì‘
    return x * 2

# 3. assertë¡œ ì¡°ê±´ ê²€ì¦
assert len(data) > 0, "Data is empty!"

# 4. try-exceptë¡œ ì˜ˆì™¸ ì²˜ë¦¬
try:
    result = risky_operation()
except Exception as e:
    print(f"Error: {e}")
    import traceback
    traceback.print_exc()
```

### 3. ì„±ëŠ¥ ìµœì í™” íŒ

```python
# 1. List comprehension ì‚¬ìš©
# ëŠë¦¼
result = []
for i in range(1000):
    result.append(i ** 2)

# ë¹ ë¦„
result = [i ** 2 for i in range(1000)]

# 2. Generator ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
def generate_squares(n):
    for i in range(n):
        yield i ** 2

# 3. ë‚´ì¥ í•¨ìˆ˜ í™œìš©
# ëŠë¦¼
result = []
for item in data:
    result.append(item * 2)

# ë¹ ë¦„
result = list(map(lambda x: x * 2, data))

# 4. setì„ í™œìš©í•œ ì¡°íšŒ
# ëŠë¦¼ - O(n)
if item in my_list:
    pass

# ë¹ ë¦„ - O(1)
if item in my_set:
    pass
```

---

## ê²°ë¡ 

Python íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì˜ í•µì‹¬:

1. **ê°€ìƒí™˜ê²½ ì‚¬ìš©**: í”„ë¡œì íŠ¸ë³„ë¡œ ë…ë¦½ì ì¸ í™˜ê²½ ìœ ì§€
2. **ë¡œê¹… í™œìš©**: print ëŒ€ì‹  logging ëª¨ë“ˆ ì‚¬ìš©
3. **í”„ë¡œíŒŒì¼ë§**: ì„±ëŠ¥ ë¬¸ì œëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ì¸¡ì •
4. **ì˜ˆì™¸ ì²˜ë¦¬**: ì ì ˆí•œ try-exceptë¡œ ì—ëŸ¬ í•¸ë“¤ë§
5. **ì½”ë“œ ë¦¬ë·°**: pylint, flake8 ë“±ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ ìœ ì§€

Happy Python Coding! ğŸ
